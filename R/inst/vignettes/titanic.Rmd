---
title: "Titanic"
author: "Paul Hendricks"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Titanic}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Overview

In this vignette, we demonstrate the power of `easyml` using the popular Titanic dataset. 

## Load the data

Install the `titanic` package from CRAN. This package contains datasets providing information on the fate of passengers on the fatal maiden voyage of the ocean liner "Titanic", with variables such as economic status (class), sex, age and survival. These data sets are often used as an introduction to machine learning on [Kaggle](https://www.kaggle.com/c/titanic). More details about the dataset can be found there.

```{r}
library(easyml)
library(titanic)
library(ggplot2)

data("titanic_train", package = "titanic")
knitr::kable(head(titanic_train))
```

## Tidy the data

To prepare the data for modeling, we will undergo the following steps:

1) Filter out any places where `Embarked` is `NA`, 
2) Add together `SibSp`, `Parch`, and `1L` to estimate family size,
3) Create binary variables for each of the 2nd and 3rd class memberships, 
4) Create a binary for gender, 
5) Create binary variables for 2 of the ports of embarkation, 
6) Impute mean values of age where `Age` is `NA`.

```{r}
titanic_train_2 <- titanic_train %>% 
  filter(!is.na(Embarked), Embarked != "") %>% 
  mutate(Family_Size = SibSp + Parch + 1L) %>% 
  mutate(Pclass = as.character(Pclass)) %>% 
  mutate(Pclass_2 = 1 * (Pclass == "2")) %>% 
  mutate(Pclass_3 = 1 * (Pclass == "3")) %>% 
  mutate(Sex = 1 * (Sex == "male")) %>% 
  mutate(Embarked_Q = 1 * (Embarked == "Q")) %>% 
  mutate(Embarked_S = 1 * (Embarked == "S")) %>% 
  mutate(Age = ifelse(is.na(Age), mean(Age, na.rm = TRUE), Age))
```

## Train the models

To run an `easy_glmnet` model, we pass in the following parameters:

* the data set `titanic_train_2`,
* the name of the dependent variable e.g. `Survived`,
* whether to run a `gaussian` or a `binomial` model, 
* how to preprocess the data; in this case, we use `preprocess_scale` to scale the data, 
* which variables to exclude from the analysis, 
* which variables are categorical variables; these variables are not scaled, if `preprocess_scale` is used, 
* the random state, 
* whether to display a progress bar, 
* how many cores to run the analysis on in parallel.

```{r}
.exclude_variables <- c("PassengerId", "Pclass", "Name", 
                        "Ticket", "Cabin", "Embarked")
.categorical_variables <- c("Sex", "SibSp", "Parch", "Family_Size", "Pclass_2", 
                            "Pclass_3", "Embarked_Q", "Embarked_S")
glmnet_results <- easy_glmnet(titanic_train_2, "Survived", 
                              family = "binomial", 
                              preprocess = preprocess_scale, 
                              exclude_variables = .exclude_variables, 
                              categorical_variables = .categorical_variables, 
                              random_state = 12345, progress_bar = FALSE, 
                              n_core = 8)
```

## Assess results

Now let's assess the results of the `easy_glmnet` model.

### Estimates of weights

We can interpret the weights in the following way:

* A 1 standard deviation increase in `Fare` increases the log-odds of survival by 0.14 units, 
* A 1 standard deviation increase in `Age` decreases the log-odds of survival by 0.47 units, 
* If a passenger embarked at the Southampton port, the log-odds of survival decrease by 0.40 units, 
* If a passenger is third class, the log-odds of survival decrease by 1.98 units, 
* If a passenger is second class, the log-odds of survival decrease by 0.77 units, 
* For every unit increase in a passenger's family size, the log-odds of survival decrease by 0.10 units, 
* For every additional Sibling or Spouse in a passenger's family, the log-odds of survival decrease by 0.21 units, 
* If a passenger is male, the log-odds of survival decrease by 2.64 units. 

```{r message=FALSE}
glmnet_results$plot_coefficients_processed
output <- glmnet_results$coefficients_processed
knitr::kable(output[nrow(output):1, ], digits  = 2)
```

### Predictions: ROC Curve

We can examine both the in-sample and out-of-sample ROC curve plots for one particular trian-test split determined by the random state and determine the Area Under the Curve (AUC) as a goodness of fit metric. Here, we see that the in-sample AUC is higher than the out-of-sample AUC, but that both metrics indicate the model fits relatively well.

```{r}
glmnet_results$plot_predictions_train_mean
glmnet_results$plot_predictions_test_mean
```

### Metrics: AUC

We can examine both the in-sample and out-of-sample AUC metrics for `n_divisions` train-test splits (ususally defaults to 1,000). Again, we see that the in-sample AUC is higher than the out-of-sample AUC, but that both metrics indicate the model fits relatively well.

```{r}
glmnet_results$plot_metrics_train_mean
glmnet_results$plot_metrics_test_mean
```

## Discuss

In this vignette we used `easyml` to easily build and evaluate a penalized binomial regression model to assess the likelihood of passenger surival given a number of attributes. We can continue to finetune the model and identify the most optimal alpha/lambda hyperparameter combination; however, our estimates of the weights make intutive sense and a mean out-of-sample AUC of 0.85 right off the bat is indicative of a good model. 
